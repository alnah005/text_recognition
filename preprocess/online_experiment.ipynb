{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from skimage import io as skimio\n",
    "from skimage import color as skimcolor\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from segmentation.imageModifiers import *\n",
    "from segmentation.plottingFuncs import *\n",
    "from segmentation.projEdgeBreaks import *\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (15, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in all classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv_fold = \"../data/ASM/\"\n",
    "if not os.path.isdir(sv_fold+\"Images\"):\n",
    "    os.mkdir(sv_fold+\"Images\")\n",
    "\n",
    "full_sv = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "full_sv = full_sv + \"data/ASM/Images/\"\n",
    "\n",
    "subjFile = sv_fold + \"anti-slavery-manuscripts-subjects.csv\"\n",
    "subj = pd.read_csv(subjFile, low_memory=False)\n",
    "\n",
    "classFile = sv_fold + \"anti-slavery-manuscripts-classifications.csv\"\n",
    "clas = pd.read_csv(classFile, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out records that haven't been classified or are in the wrong workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subj = subj[subj.classifications_count > 0]\n",
    "subj = subj[subj[\"subject_set_id\"] == 15582]\n",
    "subj = subj[subj[\"workflow_id\"] == 5329]\n",
    "clas = clas[clas.subject_ids.isin(subj.subject_id)]\n",
    "\n",
    "# remove everything before 2018 january 23 (full launch)\n",
    "dt_frmt = \"%Y-%m-%d  %H:%M:%S UTC\"\n",
    "clas[\"created_dt\"] = [datetime.strptime(i, dt_frmt) for i in clas[\"created_at\"]]\n",
    "clas = clas[clas[\"created_dt\"] > datetime(2018, 1, 22)]\n",
    "subj = subj[subj[\"subject_id\"].isin(clas[\"subject_ids\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change json columns to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subj[\"metadata_dict\"] = [json.loads(q) for q in subj[\"metadata\"]]\n",
    "subj[\"locations_dict\"] = [json.loads(q) for q in subj[\"locations\"]]\n",
    "\n",
    "clas[\"metadata_dict\"] = [json.loads(q) for q in clas[\"metadata\"]]\n",
    "clas[\"annotations_dict\"] = [json.loads(q) for q in clas[\"annotations\"]]\n",
    "clas[\"subject_data_dict\"] = [json.loads(q) for q in clas[\"subject_data\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganize them so I only have the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load input_shape from file output by preprocess\n",
    "with open(\"../data/combined/img_size.txt\", \"r\") as f:\n",
    "    doc = f.readline()\n",
    "    w, h = doc.split(\",\")\n",
    "    maxh = int(float(h))\n",
    "\n",
    "def get_transcription_lines(pts, maxh=maxh):\n",
    "    data_mini = pd.DataFrame(columns=[\"transcription\", \"x1\", \"x2\", \"y1\", \"y2\"])\n",
    "    \n",
    "    # get basic x and y information and make sure x is before y\n",
    "    for p in pts:\n",
    "        trans = p[\"details\"][0][\"value\"]\n",
    "        xy1 = (p[\"points\"][0][\"x\"], p[\"points\"][0][\"y\"])\n",
    "        xy2 = (p[\"points\"][1][\"x\"], p[\"points\"][1][\"y\"])\n",
    "        xy = [xy1, xy2]\n",
    "        xy.sort()\n",
    "        newdata1 = {\"transcription\": trans,\n",
    "                   \"x1\": xy[0][0], \"x2\": xy[1][0],\n",
    "                   \"y1\": xy[0][1], \"y2\": xy[1][1]}\n",
    "        newdata1 = pd.DataFrame.from_records([newdata1])\n",
    "        data_mini = data_mini.append(newdata1)\n",
    "    \n",
    "    # ignore all cross writing or weird lines \n",
    "    # i.e. everything where $\\Delta x < \\Delta$y\n",
    "    good_lines = abs(data_mini[\"x1\"]-data_mini[\"x2\"]) > abs(data_mini[\"y1\"]-data_mini[\"y2\"])\n",
    "    data_mini = data_mini[good_lines]\n",
    "    data_mini = data_mini.sort_values(\"y1\")\n",
    "    \n",
    "    if len(data_mini) <= 0:\n",
    "        return data_mini\n",
    "    \n",
    "    # get bounding boxes using previous line's y information\n",
    "    # for first line, use average line height\n",
    "    if len(data_mini) > 1:\n",
    "        y3 = list(data_mini[\"y1\"].copy())\n",
    "        y3_ave = np.max(np.abs(np.diff(y3)))\n",
    "        y3 = y3[:-1]\n",
    "        y3.insert(0, max(y3[0]-y3_ave, 0))\n",
    "\n",
    "        y4 = list(data_mini[\"y2\"].copy())\n",
    "        y4_ave = np.max(np.abs(np.diff(y4)))\n",
    "        y4 = y4[:-1]\n",
    "        y4.insert(0, max(y4[0]-y4_ave, 0))\n",
    "    else: # if there's only one line, guess based on my model's max height\n",
    "        y3 = [max(data_mini[\"y1\"].iloc[0]-maxh, 0)]\n",
    "        y4 = [max(data_mini[\"y2\"].iloc[0]-maxh, 0)]\n",
    "    \n",
    "    data_mini[\"y3\"] = y3\n",
    "    data_mini[\"y4\"] = y4\n",
    "    \n",
    "    return data_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 21243 items...\n",
      "0\t1000\t2000\t3000\t4000\t5000\t6000\t7000\t8000\t9000\t10000\t11000\t12000\t13000\t14000\t15000\t16000\t17000\t18000\t19000\t20000\t21000\t\n",
      "Created 30656 data entries and saved file\n"
     ]
    }
   ],
   "source": [
    "redo = True\n",
    "\n",
    "if os.path.exists(\"../data/ASM/newclas.pkl\") and not redo:\n",
    "    data = pickle.load(open(\"../data/ASM/newclas.pkl\", \"rb\"))\n",
    "    print(\"\\n{0} data entries loaded\".format(len(data)))\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"subject_id\", \"classification_id\", \"workflow_id\", \"frame\", \"created_dt\", \n",
    "                                 \"annotations\", \"metadata_clas\", \"metadata_subj\", \"location\", \"annodata\"])\n",
    "\n",
    "    print(\"Processing\", len(clas), \"items...\")\n",
    "    for i in range(len(clas)):\n",
    "        clas1 = clas.iloc[i]\n",
    "        subj1 = subj[subj[\"subject_id\"] == clas1[\"subject_ids\"]]\n",
    "\n",
    "        val = clas1[\"annotations_dict\"][0][\"value\"]\n",
    "        frames = np.sort(np.unique([v[\"frame\"] for v in val]))\n",
    "\n",
    "        for fr in frames:\n",
    "            fr = frames[0]\n",
    "            pts = [v for v in val if v[\"frame\"] == fr]\n",
    "\n",
    "            data_mini = get_transcription_lines(pts)\n",
    "\n",
    "            newdata = {\"subject_id\": clas1[\"subject_ids\"],\n",
    "                       \"classification_id\": clas1[\"classification_id\"],\n",
    "                       \"workflow_id\": clas1[\"workflow_id\"],\n",
    "                       \"frame\": fr,\n",
    "                       \"created_dt\": clas1[\"created_dt\"],\n",
    "                       \"annotations\": clas1[\"annotations\"],\n",
    "                       \"metadata_clas\": [clas1[\"metadata_dict\"]],\n",
    "                       \"metadata_subj\": [subj1[\"metadata_dict\"]],\n",
    "                       \"location\": subj1[\"locations_dict\"].iloc[0][str(fr)],\n",
    "                       \"annodata\": [data_mini]}\n",
    "            newdata = pd.DataFrame.from_dict(newdata)\n",
    "            data = data.append(newdata)\n",
    "        if i % 1000 == 0: print(i, end=\"\\t\")\n",
    "    data.sort_values(\"created_dt\")\n",
    "    pickle.dump(data, open(\"../data/ASM/newclas.pkl\", \"wb\"))\n",
    "    print(\"\\nCreated {0} data entries and saved file\".format(len(data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo = False\n",
    "\n",
    "if os.path.exists(\"../data/ASM/newclas.pkl\") and not redo:\n",
    "    data = pickle.load(open(\"../data/ASM/newclas.pkl\", \"rb\"))\n",
    "    print(\"\\n{0} frames loaded in data file\".format(len(data)))\n",
    "else:\n",
    "    data = pd.DataFrame(columns=[\"subject_id\", \"classification_id\", \"workflow_id\", \"frame\", \"created_dt\", \n",
    "                                 \"annotations\", \"metadata_clas\", \"metadata_subj\", \"location\", \"annodata\"])\n",
    "\n",
    "    print(\"Processing\", len(clas), \"items...\")\n",
    "    for i in range(len(clas)):\n",
    "        clas1 = clas.iloc[i]\n",
    "        subj1 = subj[subj[\"subject_id\"] == clas1[\"subject_ids\"]]\n",
    "\n",
    "        val = clas1[\"annotations_dict\"][0][\"value\"]\n",
    "        frames = np.sort(np.unique([v[\"frame\"] for v in val]))\n",
    "\n",
    "        for fr in frames:\n",
    "            fr = frames[0]\n",
    "            pts = [v for v in val if v[\"frame\"] == fr]\n",
    "\n",
    "            data_mini = pd.DataFrame(columns=[\"transcription\", \"x1\", \"x2\", \"y1\", \"y2\"])\n",
    "            for p in pts:\n",
    "                trans = p[\"details\"][0][\"value\"]\n",
    "                xy1 = (p[\"points\"][0][\"x\"], p[\"points\"][0][\"y\"])\n",
    "                xy2 = (p[\"points\"][1][\"x\"], p[\"points\"][1][\"y\"])\n",
    "                xy = [xy1, xy2]\n",
    "                xy.sort()\n",
    "                newdata1 = {\"transcription\": trans,\n",
    "                           \"x1\": xy[0][0], \"x2\": xy[1][0],\n",
    "                           \"y1\": xy[0][1], \"y2\": xy[1][1]}\n",
    "                newdata1 = pd.DataFrame.from_records([newdata1])\n",
    "                data_mini = data_mini.append(newdata1)\n",
    "            data_mini = data_mini.sort_values(\"y1\")\n",
    "\n",
    "\n",
    "            newdata = {\"subject_id\": clas1[\"subject_ids\"],\n",
    "                       \"classification_id\": clas1[\"classification_id\"],\n",
    "                       \"workflow_id\": clas1[\"workflow_id\"],\n",
    "                       \"frame\": fr,\n",
    "                       \"created_dt\": clas1[\"created_dt\"],\n",
    "                       \"annotations\": clas1[\"annotations\"],\n",
    "                       \"metadata_clas\": [clas1[\"metadata_dict\"]],\n",
    "                       \"metadata_subj\": [subj1[\"metadata_dict\"]],\n",
    "                       \"location\": subj1[\"locations_dict\"].iloc[0][str(fr)],\n",
    "                       \"annodata\": [data_mini]}\n",
    "            newdata = pd.DataFrame.from_dict(newdata)\n",
    "            data = data.append(newdata)\n",
    "        if i % 1000 == 0: print(i, end=\"\\t\")\n",
    "    data.sort_values(\"created_dt\")\n",
    "    pickle.dump(data, open(\"../data/ASM/newclas.pkl\", \"wb\"))\n",
    "    print(\"\\n{0} frames processed and file saved\".format(len(data)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to ignore all cross writing or weird lines i.e. everything where $\\Delta x < \\Delta$y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input_shape from file output by preprocess\n",
    "with open(\"../data/combined/img_size.txt\", \"r\") as f:\n",
    "    doc = f.readline()\n",
    "    w, h = doc.split(\",\")\n",
    "    maxh = int(float(h))\n",
    "\n",
    "data.index = list(range(len(data)))\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        print(i, \"0\", end=\"\\t\")\n",
    "        data_mini = data[\"annodata\"].iloc[i].copy()[0]\n",
    "    except:\n",
    "        print(i, \"not0\", end=\"\\t\")\n",
    "        data_mini = data[\"annodata\"].iloc[i].copy()\n",
    "        \n",
    "    \n",
    "    good_lines = abs(data_mini[\"x1\"]-data_mini[\"x2\"]) > abs(data_mini[\"y1\"]-data_mini[\"y2\"])\n",
    "    data_mini = data_mini[good_lines]\n",
    "    data_mini = data_mini.sort_values(\"y1\")\n",
    "    if len(data_mini) <= 0:\n",
    "        continue\n",
    "    if len(data_mini) > 1:\n",
    "        y3 = list(data_mini[\"y1\"].copy())\n",
    "        y3_ave = np.max(np.abs(np.diff(y3)))\n",
    "        y3 = y3[:-1]\n",
    "        y3.insert(0, max(y3[0]-y3_ave, 0)) # use average height of line for first value\n",
    "\n",
    "        y4 = list(data_mini[\"y2\"].copy())\n",
    "        y4_ave = np.max(np.abs(np.diff(y4)))\n",
    "        y4 = y4[:-1]\n",
    "        y4.insert(0, max(y4[0]-y4_ave, 0)) # use average height of line for first value\n",
    "    else:\n",
    "        y3 = [max(data_mini[\"y1\"].iloc[0]-maxh, 0)]\n",
    "        y4 = [max(data_mini[\"y2\"].iloc[0]-maxh, 0)]\n",
    "    \n",
    "    data_mini[\"y3\"] = y3\n",
    "    data_mini[\"y4\"] = y4\n",
    "    \n",
    "    data.at[i, \"annodata\"] = [data_mini]\n",
    "    if i % 1000 == 0: print(i, end=\"\\t\")\n",
    "print(\"\\nAll data processed and file saved\")\n",
    "data.sort_values(\"created_dt\")\n",
    "pickle.dump(data, open(\"../data/ASM/newclas.pkl\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess by splitting image into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readImg(url, grey=True):\n",
    "    response = requests.get(loc)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.convert(\"LA\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = readImg(loc)\n",
    "# Image.open(loc)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = clas1[\"annotations_dict\"][0][\"value\"]\n",
    "val\n",
    "for v in val:\n",
    "    if v[\"frame\"] == 0:\n",
    "        pts = v[\"points\"]\n",
    "        print(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in subj1[\"locations_dict\"].iloc[0].keys():\n",
    "    val = clas1[\"annotations_dict\"][0][\"value\"]\n",
    "    pts = [v[\"points\"] for v in val if v[\"frame\"] == k]\n",
    "    \n",
    "    print(k)\n",
    "# clas1.annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training csv w/ transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction current section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on current section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all images to save space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
