{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original repo: https://github.com/solivr/tf-crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the details to make all images the same size\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from skimage import io as skimio\n",
    "from skimage import color as skimcolor\n",
    "import skimage.transform as skimtrans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "local_path = \"C:/Users/danny/Repos/text_recognition/tf-crnn-master/\"\n",
    "img_dir_old = \"data/Images/\"\n",
    "img_dir_new = \"data/Images_mod/\"\n",
    "trans_dir = \"data/Transcriptions/\"\n",
    "\n",
    "filenames = [f.replace(\".txt\", \"\") for f in os.listdir(trans_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad images\n",
    "To do: pad in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = []\n",
    "count = 0\n",
    "for fn in filenames:\n",
    "    im = skimio.imread(img_dir_old + fn + \".png\")\n",
    "    sizes.append(im.shape)\n",
    "    if count % 1000 == 0: print(count, end = \"\\t\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights = [s[0] for s in sizes]\n",
    "widths = [s[1] for s in sizes]\n",
    "max_height = np.max(heights)\n",
    "max_width = np.max(widths)\n",
    "print(max_height, max_width)\n",
    "med_height = np.median(heights)\n",
    "med_width = np.median(widths)\n",
    "print(med_height, med_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the image file\n",
    "def readBenthamImg(fn, ftype=\".png\", img_dir_new=img_dir_new,\n",
    "                   img_dir_old=img_dir_old, goal_size = (med_height, med_width)):\n",
    "    im = skimio.imread(img_dir_old + fn + ftype)\n",
    "    # turn all transparent pixels to background (average)\n",
    "    rep_val = max(np.median(im[:,:,0]), np.mean(im[:,:,0]))\n",
    "    im[im[:,:,3] == 0] = [rep_val, rep_val, rep_val, 255]\n",
    "    im = skimcolor.rgb2gray(im)*255\n",
    "\n",
    "    # make all images the same size\n",
    "    size = im.shape\n",
    "    if int(1.0*goal_size[1]/size[1]*size[0]) < goal_size[0]:\n",
    "        new_size = (round(1.0*goal_size[1]/size[1]*size[0]), goal_size[1])\n",
    "        pad_size = ((0, int(goal_size[0] - new_size[0])), (0,0))\n",
    "    elif int(1.0*goal_size[0]/size[0]*size[1]) < goal_size[1]:\n",
    "        new_size = (goal_size[0], round(1.0*goal_size[0]/size[0]*size[1]))\n",
    "        pad_size = ((0,0), (0, int(goal_size[1] - new_size[1])))\n",
    "    elif all(np.equal(size, goal_size)):\n",
    "        new_size = goal_size\n",
    "        pad_size = ((0,0), (0,0))\n",
    "    # resize and pad based on previous calculation\n",
    "    im = skimtrans.resize(im, new_size, mode=\"constant\")\n",
    "    im = np.pad(im, pad_size, mode=\"constant\", constant_values=rep_val)\n",
    "    im = im.astype(\"int\")\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        skimio.imsave(img_dir_new + fn + ftype, im)\n",
    "    return img_dir_new + fn + ftype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Partitions/TrainLines.lst\") as f:\n",
    "    training = f.read().splitlines()\n",
    "\n",
    "with open(\"data/Partitions/ValidationLines.lst\") as f:\n",
    "    validation = f.read().splitlines()\n",
    "\n",
    "with open(\"data/Partitions/TestLines.lst\") as f:\n",
    "    test = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for fn in filenames:\n",
    "    new_fn = readBenthamImg(fn)\n",
    "    if count % 1000 == 0: print(count, end = \"\\t\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transcription csvs\n",
    "Deal with utf-8 compliance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the transcription\n",
    "def readBenthamTrans(fn, ftype=\".txt\", trans_dir=trans_dir):\n",
    "    with open(trans_dir + fn + ftype, \"r\") as f:\n",
    "        trans = f.readline()\n",
    "        trans = trans.replace(\"\\n\", \"\")\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the alphabet\n",
    "count = 0\n",
    "alphabet = dict()\n",
    "for fn in filenames:\n",
    "    trans = readBenthamTrans(fn)\n",
    "    for l in list(trans):\n",
    "        if l not in alphabet:\n",
    "            alphabet[l] = 0\n",
    "        alphabet[l] += 1\n",
    "    if count % 1000 == 0: print(count, end=\"\\t\")\n",
    "    count += 1\n",
    "\n",
    "print()\n",
    "alpha_string1 = \"\".join(np.sort(list(alphabet)))\n",
    "print(alpha_string1)\n",
    "\n",
    "alphabet2 = {k for k, v in alphabet.items() if v > 1}\n",
    "alpha_string2 = \"\".join(np.sort(list(alphabet2)))\n",
    "print(alpha_string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in alpha_string1:\n",
    "    print(l, ord(l), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/Partitions/TrainLines.lst\") as f:\n",
    "    training = f.read().splitlines()\n",
    "\n",
    "with open(\"data/Partitions/ValidationLines.lst\") as f:\n",
    "    validation = f.read().splitlines()\n",
    "\n",
    "with open(\"data/Partitions/TestLines.lst\") as f:\n",
    "    test = f.read().splitlines()\n",
    "\n",
    "#local_path = \"/home/danny/Repos/text_recognition/tf-crnn-master/\"\n",
    "count = 0\n",
    "with open(\"data/train.csv\", \"w\") as f_tr, open(\"data/valid.csv\", \"w\") as f_va, open(\"data/test.csv\", \"w\") as f_te:\n",
    "    for fn in filenames:\n",
    "        new_fn = local_path + img_dir_new + fn + \".png\"\n",
    "        trans = readBenthamTrans(fn)\n",
    "        trans = trans.replace('\"', '\"\"')\n",
    "        if any([ord(i) > 127 for i in trans]):\n",
    "            continue\n",
    "        if fn in training:\n",
    "            f_tr.write(\"{0}\\t\\\"{1}\\\"\\n\".format(new_fn, trans))\n",
    "        elif fn in validation:\n",
    "            f_va.write(\"{0}\\t\\\"{1}\\\"\\n\".format(new_fn, trans))\n",
    "        elif fn in test:\n",
    "            f_te.write(\"{0}\\t\\\"{1}\\\"\\n\".format(new_fn, trans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
