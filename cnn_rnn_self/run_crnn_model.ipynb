{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danny/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "# from tqdm import trange\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "# from .decoding import get_words_from_chars\n",
    "# from .config import Params, CONST\n",
    "# from src.model import crnn_fn\n",
    "# from src.data_handler import data_loader\n",
    "# from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "# from src.config import Params, Alphabet, import_params_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains information for the actual network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_files_train = \"/home/danny/Repos/text_recognition/tf-crnn-master/data/train.csv\"\n",
    "csv_files_eval = \"/home/danny/Repos/text_recognition/tf-crnn-master//data/valid.csv\"\n",
    "output_model_dir = \"/home/danny/Repos/text_recognition/tf-crnn-master/estimator\"\n",
    "# csv_files_train = \"C:/Users/danny/Repos/text_recognition/tf-crnn-master/data/train.csv\"\n",
    "# csv_files_eval = \"C:/Users/danny/Repos/text_recognition/tf-crnn-master//data/valid.csv\"\n",
    "# output_model_dir = \"C:/Users/danny/Repos/text_recognition/tf-crnn-master/estimator\"\n",
    "n_epochs = 5\n",
    "gpu = \"\" # help=\"GPU 0,1 or '' \", default=''\n",
    "\n",
    "train_batch_size=64\n",
    "eval_batch_size=64\n",
    "learning_rate=1e-3  # 1e-3 recommended\n",
    "learning_decay_rate=0.95\n",
    "learning_decay_steps=5000\n",
    "evaluate_every_epoch=5\n",
    "save_interval=5e3\n",
    "input_shape=(117, 1669)\n",
    "optimizer='adam'\n",
    "digits_only=False\n",
    "alphabet=\" !\\\"#&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|~\"\n",
    "alphabet_decoding='same'\n",
    "alphabet_codes = list(range(len(alphabet)))\n",
    "n_classes = len(alphabet)\n",
    "csv_delimiter='\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed for quickly making convolutional layers\n",
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob_dropout = 0.7\n",
    "#input_tensor = features['images']\n",
    "input_tensor = tf.placeholder(tf.float32, [None, input_shape[0], input_shape[1], 1])\n",
    "labels = tf.placeholder(tf.string, [None])\n",
    "is_training = True\n",
    "\n",
    "if input_tensor.shape[-1] == 1:\n",
    "    input_channels = 1\n",
    "elif input_tensor.shape[-1] == 3:\n",
    "    input_channels = 3\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('deep_cnn'):\n",
    "    # conv1 - maxPool2x2\n",
    "    with tf.variable_scope('layer1'):\n",
    "        W = weightVar([3, 3, input_channels, 64])\n",
    "        b = biasVar([64])\n",
    "        conv = conv2d(input_tensor, W, name='conv')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv1 = tf.nn.relu(out)\n",
    "        pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "\n",
    "    # conv2 - maxPool 2x2\n",
    "    with tf.variable_scope('layer2'):\n",
    "        W = weightVar([3, 3, 64, 128])\n",
    "        b = biasVar([128])\n",
    "        conv = conv2d(pool1, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv2 = tf.nn.relu(out)\n",
    "        pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "    # conv3 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer3'):\n",
    "        W = weightVar([3, 3, 128, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(pool2, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "    # conv4 - maxPool 2x1\n",
    "    with tf.variable_scope('layer4'):\n",
    "        W = weightVar([3, 3, 256, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(conv3, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv4 = tf.nn.relu(out)\n",
    "        pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool4')\n",
    "\n",
    "    # conv5 - w/batch-norm\n",
    "    with tf.variable_scope('layer5'):\n",
    "        W = weightVar([3, 3, 256, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool4, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # conv6 - maxPool 2x1 (as source code, not paper)\n",
    "    with tf.variable_scope('layer6'):\n",
    "        W = weightVar([3, 3, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(conv5, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv6 = tf.nn.relu(out)\n",
    "        pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool6')\n",
    "\n",
    "    # conv 7 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer7'):\n",
    "        W = weightVar([2, 2, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool6, W, padding='VALID')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # reshape output\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = conv7.get_shape().as_list()  # [batch, height, width, features]\n",
    "        shape_tens = tf.shape(conv7)\n",
    "        transposed = tf.transpose(conv7, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_out = tf.reshape(transposed, [shape_tens[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logprob, raw_pred = deep_bidirectional_lstm(conv, params=parameters, summaries=False)\n",
    "\n",
    "# def deep_bidirectional_lstm(inputs: tf.Tensor, params: Params, summaries: bool=True) -> tf.Tensor:\n",
    "# Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "# Current data input shape: (batch_size, n_steps, n_input) \"(batch, time, height)\"\n",
    "\n",
    "list_n_hidden = [256, 256]\n",
    "\n",
    "with tf.name_scope('deep_bidirectional_lstm'):\n",
    "    # Forward direction cells\n",
    "    fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "    # Backward direction cells\n",
    "    bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "    lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                    bw_cell_list,\n",
    "                                                                    conv_out, # THE INPUT\n",
    "                                                                    dtype=tf.float32\n",
    "                                                                    )\n",
    "\n",
    "    # Dropout layer\n",
    "    lstm_net = tf.nn.dropout(lstm_net, keep_prob=keep_prob_dropout)\n",
    "\n",
    "    with tf.variable_scope('Reshaping_rnn'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]\n",
    "\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        W = weightVar([list_n_hidden[-1]*2, n_classes])\n",
    "        b = biasVar([n_classes])\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "\n",
    "    shape_tens = tf.shape(lstm_net)\n",
    "    lstm_out = tf.reshape(fc_out, [shape_tens[0], -1, n_classes], name='reshape_out')  # [batch, width, n_classes]\n",
    "\n",
    "    raw_pred = tf.argmax(tf.nn.softmax(lstm_out), axis=2, name='raw_prediction')\n",
    "\n",
    "    # Swap batch and time axis\n",
    "    lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CNN and LSTM and create training op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for loss and training\n",
    "\n",
    "# Compute seq_len from image width\n",
    "n_pools = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide([input_shape[1]]*train_batch_size, n_pools,\n",
    "                           name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': lstm_out,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "# Get keys (letters) and values (integer stand ins for letters)\n",
    "# Alphabet and codes\n",
    "keys = [c for c in alphabet] # the letters themselves\n",
    "values = alphabet_codes # integer representations\n",
    "\n",
    "\n",
    "# Create non-string labels from the keys and values above\n",
    "# Convert string label to code label\n",
    "with tf.name_scope('str2code_conversion'):\n",
    "    table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "    splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "    codes = table_str2int.lookup(splited.values)\n",
    "    sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                 minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "\n",
    "# Use ctc loss on probabilities from lstm output\n",
    "# Loss\n",
    "# ----\n",
    "# >>> Cannot have longer labels than predictions -> error\n",
    "with tf.control_dependencies([tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "    loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                              inputs=predictions_dict['prob'],\n",
    "                              sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                              preprocess_collapse_repeated=False,\n",
    "                              ctc_merge_repeated=True,\n",
    "                              ignore_longer_outputs_than_inputs=True,  # returns zero gradient in case it happens -> ema loss = NaN\n",
    "                              time_major=True)\n",
    "    loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "    loss_ctc = tf.Print(loss_ctc, [loss_ctc], message='* Loss : ')\n",
    "\n",
    "    \n",
    "# Create the learning rate as well as a moving average\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# # Create an ExponentialMovingAverage object\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=global_step, zero_debias=True)\n",
    "# Create the shadow variables, and add op to maintain moving averages\n",
    "maintain_averages_op = ema.apply([loss_ctc])\n",
    "loss_ema = ema.average(loss_ctc)\n",
    "\n",
    "# Train op\n",
    "# --------\n",
    "learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                           learning_decay_steps, learning_decay_rate,\n",
    "                                           staircase=True)\n",
    "\n",
    "\n",
    "# Set up optimizer\n",
    "if optimizer == 'ada':\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "elif optimizer == 'rms':\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "opt_op = optimizer.minimize(loss_ctc, global_step=global_step)\n",
    "with tf.control_dependencies(update_ops + [opt_op]):\n",
    "    train_op = tf.group(maintain_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "There's an issue with unicode so I'm going to (in a different file) remove all multi-character utf-8 characters  \n",
    "https://github.com/tensorflow/tensorflow/issues/11399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the details to make all images the same size\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from skimage import io as skimio\n",
    "from skimage import color as skimcolor\n",
    "import skimage.transform as skimtrans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "local_path = \"/home/danny/Repos/text_recognition/tf-crnn-master/\"\n",
    "img_dir = \"data/Images_mod/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/train.csv\", \"r\") as f:\n",
    "    f_lines = f.read().splitlines()\n",
    "    filenames = [l.split(\"\\t\")[0] for l in f_lines]\n",
    "    label_list = [l.split(\"\\t\")[1][1:-1] for l in f_lines]\n",
    "datasize = len(label_list)\n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize_images(image_decoded, input_shape)\n",
    "    return image_decoded, label\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, label_list))\n",
    "dataset = dataset.map(_parse_function).batch(train_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting batch\n",
      "Got data\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Saw a non-null label (index >= num_classes - 1) following a null label, batch: 9 num_classes: 86 labels: 30,75,77,12,16,0,12,0,49,65,62,0,76,58,66,61,0,76,78,70,0,72,63,0,0\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](deep_bidirectional_lstm/transpose_time_major, str2code_conversion/StringSplit, str2code_conversion/hash_table_Lookup, Cast_3)]]\n\nCaused by op 'CTCLoss', defined at:\n  File \"/home/danny/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/danny/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-b0cc214b2b61>\", line 42, in <module>\n    time_major=True)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/ctc_ops.py\", line 156, in ctc_loss\n    ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_ctc_ops.py\", line 212, in _ctc_loss\n    name=name)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\n    op_def=op_def)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Saw a non-null label (index >= num_classes - 1) following a null label, batch: 9 num_classes: 86 labels: 30,75,77,12,16,0,12,0,49,65,62,0,76,58,66,61,0,76,78,70,0,72,63,0,0\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](deep_bidirectional_lstm/transpose_time_major, str2code_conversion/StringSplit, str2code_conversion/hash_table_Lookup, Cast_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 9 num_classes: 86 labels: 30,75,77,12,16,0,12,0,49,65,62,0,76,58,66,61,0,76,78,70,0,72,63,0,0\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](deep_bidirectional_lstm/transpose_time_major, str2code_conversion/StringSplit, str2code_conversion/hash_table_Lookup, Cast_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-aeb75f83dc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput_tensor_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtr_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_tensor_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_b\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 9 num_classes: 86 labels: 30,75,77,12,16,0,12,0,49,65,62,0,76,58,66,61,0,76,78,70,0,72,63,0,0\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](deep_bidirectional_lstm/transpose_time_major, str2code_conversion/StringSplit, str2code_conversion/hash_table_Lookup, Cast_3)]]\n\nCaused by op 'CTCLoss', defined at:\n  File \"/home/danny/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/danny/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-b0cc214b2b61>\", line 42, in <module>\n    time_major=True)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/ctc_ops.py\", line 156, in ctc_loss\n    ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_ctc_ops.py\", line 212, in _ctc_loss\n    name=name)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3176, in create_op\n    op_def=op_def)\n  File \"/home/danny/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Saw a non-null label (index >= num_classes - 1) following a null label, batch: 9 num_classes: 86 labels: 30,75,77,12,16,0,12,0,49,65,62,0,76,58,66,61,0,76,78,70,0,72,63,0,0\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](deep_bidirectional_lstm/transpose_time_major, str2code_conversion/StringSplit, str2code_conversion/hash_table_Lookup, Cast_3)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for _ in range(n_batches):\n",
    "            print(\"Starting batch\")\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            print(\"Got data\")\n",
    "            tr_batch = sess.run(train_op, feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            print(\"batch\")\n",
    "            \n",
    "        print('epoch: {0}, loss: {1}'.format(i, tr_batch))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out error with CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up for loss and training\n",
    "\n",
    "# Compute seq_len from image width\n",
    "n_pools = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide([input_shape[1]]*train_batch_size, n_pools,\n",
    "                           name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': lstm_out,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "# Get keys (letters) and values (integer stand ins for letters)\n",
    "# Alphabet and codes\n",
    "keys = [c for c in alphabet] # the letters themselves\n",
    "values = alphabet_codes # integer representations\n",
    "\n",
    "\n",
    "# Create non-string labels from the keys and values above\n",
    "# Convert string label to code label\n",
    "with tf.name_scope('str2code_conversion'):\n",
    "    table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "    splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "    codes = table_str2int.lookup(splited.values)\n",
    "    sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                 minlength=417)#tf.shape(predictions_dict['prob'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/train.csv\", \"r\") as f:\n",
    "    f_lines = f.read().splitlines()\n",
    "    filenames = [l.split(\"\\t\")[0] for l in f_lines]\n",
    "    label_list = [l.split(\"\\t\")[1][1:-1] for l in f_lines]\n",
    "datasize = len(label_list)\n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize_images(image_decoded, input_shape)\n",
    "    return image_decoded, label\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, label_list))\n",
    "dataset = dataset.map(_parse_function).batch(train_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "n_epochs = 1\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = 1#int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for _ in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            w1, w2 = sess.run([sparse_code_target, splited], feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "#             print(w1)\n",
    "            if np.any(np.less(w1.values, 0)): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "85\n",
      "[ 9 25]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(w1.values < 0))\n",
    "print(w1.values[432])\n",
    "print(w1.indices[432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'~'\n"
     ]
    }
   ],
   "source": [
    "print(w2.values[432])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# load data\n",
    "mnist = input_data.read_data_sets('/data/mnist', one_hot=True)\n",
    "\n",
    "# set learning parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "training_steps = 1500\n",
    "training_display = 500\n",
    "\n",
    "# LSTM parameters\n",
    "hidden_size = 200\n",
    "n_classes = 10\n",
    "\n",
    "# create placeholders for features and labels\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28]) # treat it as 28x28 vector\n",
    "Y = tf.placeholder(tf.int32, [None, 10])\n",
    "\n",
    "# create weights and bias\n",
    "w = tf.Variable(tf.zeros([hidden_size, 10]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "# create LSTM layer\n",
    "little_x = tf.unstack(X, 28, 1)\n",
    "lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=1.0)\n",
    "outputs, states = rnn.static_rnn(lstm_cell, little_x, dtype=tf.float32)\n",
    "logits = tf.matmul(outputs[-1], w) + b\n",
    "\n",
    "# loss function\n",
    "soft = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "loss = tf.reduce_mean(soft)\n",
    "\n",
    "# training op\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# model evaluation\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))  \n",
    "\n",
    "\n",
    "# run optimization\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(training_steps):  # train the model training_steps times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "        X_batch = X_batch.reshape((batch_size, 28, 28))\n",
    "        sess.run(optimizer, feed_dict={X: X_batch, Y: Y_batch})\n",
    "        if i % training_display == 0:\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy], feed_dict={X: X_batch,\n",
    "                                                                            Y: Y_batch})\n",
    "            acc_batch2 = acc_batch / batch_size\n",
    "            print('step: {0}, loss: {1}, accuracy: {2}'.format(i, loss_batch, acc_batch2))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!') \n",
    "\n",
    "    # test the final model\n",
    "    n_batches = int(mnist.test.num_examples / batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        X_batch = X_batch.reshape((batch_size, 28, 28))\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y: Y_batch})\n",
    "        total_correct_preds += accuracy_batch\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds / mnist.test.num_examples))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
