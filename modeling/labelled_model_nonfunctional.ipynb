{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "#try:\n",
    "#    import better_exceptions\n",
    "#except ImportError:\n",
    "#    pass\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "from src.model import crnn_fn\n",
    "from src.data_handler import data_loader\n",
    "from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "from src.config import Params, Alphabet, import_params_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-ft', '--csv_files_train', required=True, type=str, help='CSV filename for training',\n",
    "#                     nargs='*', default=None)\n",
    "# parser.add_argument('-fe', '--csv_files_eval', type=str, help='CSV filename for evaluation',\n",
    "#                     nargs='*', default=None)\n",
    "# parser.add_argument('-o', '--output_model_dir', required=True, type=str,\n",
    "#                     help='Directory for output', default='./estimator')\n",
    "# parser.add_argument('-n', '--nb-epochs', type=int, default=30, help='Number of epochs')\n",
    "# parser.add_argument('-g', '--gpu', type=str, help=\"GPU 0,1 or '' \", default='')\n",
    "# parser.add_argument('-p', '--params-file', type=str, help='Parameters filename', default=None)\n",
    "# args = vars(parser.parse_args())\n",
    "\n",
    "csv_files_train = \"/home/danny/Repos/text_recognition/tf-crnn-master/data/train.csv\"\n",
    "csv_files_eval = \"/home/danny/Repos/text_recognition/tf-crnn-master//data/valid.csv\"\n",
    "output_model_dir = \"/home/danny/Repos/text_recognition/tf-crnn-master/estimator\"\n",
    "n_epochs = 30\n",
    "gpu = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains information for the actual network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = Params(train_batch_size=64,\n",
    "                    eval_batch_size=64,\n",
    "                    learning_rate=1e-3,  # 1e-3 recommended\n",
    "                    learning_decay_rate=0.95,\n",
    "                    learning_decay_steps=5000,\n",
    "                    evaluate_every_epoch=5,\n",
    "                    save_interval=5e3,\n",
    "                    input_shape=(117, 1669),\n",
    "                    optimizer='adam',\n",
    "                    digits_only=False,\n",
    "                    alphabet=Alphabet.MY_ALPHABET,\n",
    "                    alphabet_decoding='same',\n",
    "                    csv_delimiter='\\t',\n",
    "                    csv_files_eval=csv_files_eval,\n",
    "                    csv_files_train=csv_files_train,\n",
    "                    output_model_dir=output_model_dir,\n",
    "                    n_epochs=n_epochs,\n",
    "                    gpu=gpu\n",
    "                    )\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'Params': parameters,\n",
    "}\n",
    "\n",
    "parameters.export_experiment_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = parameters.gpu\n",
    "config_sess = tf.ConfigProto()\n",
    "config_sess.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config_sess.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the estimator (including the model, below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config estimator\n",
    "est_config = tf.estimator.RunConfig()\n",
    "est_config.replace(keep_checkpoint_max=10,\n",
    "                   save_checkpoints_steps=parameters.save_interval,\n",
    "                   session_config=config_sess,\n",
    "                   save_checkpoints_secs=None,\n",
    "                   save_summary_steps=1000,\n",
    "                   model_dir=parameters.output_model_dir)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=crnn_fn,\n",
    "                                   params=model_params,\n",
    "                                   model_dir=parameters.output_model_dir,\n",
    "                                   config=est_config\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count number of image filenames in csv\n",
    "n_samples = 0\n",
    "with open(parameters.csv_files_eval, 'r', encoding='utf8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=parameters.csv_delimiter)\n",
    "    n_samples += len(list(reader))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    for e in trange(0, parameters.n_epochs, parameters.evaluate_every_epoch):\n",
    "        estimator.train(input_fn=data_loader(csv_filename=parameters.csv_files_train,\n",
    "                                             params=parameters,\n",
    "                                             batch_size=parameters.train_batch_size,\n",
    "                                             num_epochs=parameters.evaluate_every_epoch,\n",
    "                                             data_augmentation=True,\n",
    "                                             image_summaries=True))\n",
    "        estimator.evaluate(input_fn=data_loader(csv_filename=parameters.csv_files_eval,\n",
    "                                                params=parameters,\n",
    "                                                batch_size=parameters.eval_batch_size,\n",
    "                                                num_epochs=1),\n",
    "                           steps=np.floor(n_samples/parameters.eval_batch_size)\n",
    "                           )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted')\n",
    "    estimator.export_savedmodel(os.path.join(parameters.output_model_dir, 'export'),\n",
    "                                preprocess_image_for_prediction(min_width=10))\n",
    "    print('Exported model to {}'.format(os.path.join(parameters.output_model_dir, 'export')))\n",
    "\n",
    "estimator.export_savedmodel(os.path.join(parameters.output_model_dir, 'export'),\n",
    "                            preprocess_image_for_prediction(min_width=10))\n",
    "print('Exported model to {}'.format(os.path.join(parameters.output_model_dir, 'export')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The start of the crnn, split into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def crnn_fn(features, labels, mode, params):\n",
    "\"\"\"\n",
    ":param features: dict {\n",
    "                        'images'\n",
    "                        'images_widths'\n",
    "                        'filenames'\n",
    "                        }\n",
    ":param labels: labels. flattend (1D) array with encoded label (one code per character)\n",
    ":param mode:\n",
    ":param params: dict {\n",
    "                        'Params'\n",
    "                    }\n",
    ":return:\n",
    "\"\"\"\n",
    "\n",
    "parameters = params.get('Params')\n",
    "assert isinstance(parameters, Params)\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    parameters.keep_prob_dropout = 0.7\n",
    "else:\n",
    "    parameters.keep_prob_dropout = 1.0\n",
    "\n",
    "conv = deep_cnn(features['images'], (mode == tf.estimator.ModeKeys.TRAIN), summaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The deep_cnn code (line immediately above) is in the cells below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed for quickly making convolutional layers\n",
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def deep_cnn(input_imgs: tf.Tensor, is_training: bool, summaries: bool=True) -> tf.Tensor:\n",
    "input_tensor = input_imgs\n",
    "if input_tensor.shape[-1] == 1:\n",
    "    input_channels = 1\n",
    "elif input_tensor.shape[-1] == 3:\n",
    "    input_channels = 3\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Following source code, not paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 1\n",
    "input: image tensor  \n",
    "filter: [3,3,input_channels,64]  \n",
    "bias: 64  \n",
    "activation: relu  \n",
    "pooling: [1,2,2,1], [1,2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.variable_scope('deep_cnn'):\n",
    "# - conv1 - maxPool2x2\n",
    "with tf.variable_scope('layer1'):\n",
    "    W = weightVar([3, 3, input_channels, 64])\n",
    "    b = biasVar([64])\n",
    "    conv = conv2d(input_tensor, W, name='conv')\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    conv1 = tf.nn.relu(out)\n",
    "    pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME', name='pool')\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer1/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer1/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 2\n",
    "input: pool1 from convolution 1  \n",
    "filter: [3,3,64,128]  \n",
    "bias: 128  \n",
    "activation: relu  \n",
    "pooling: [1,2,2,1], [1,2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv2 - maxPool 2x2\n",
    "with tf.variable_scope('layer2'):\n",
    "    W = weightVar([3, 3, 64, 128])\n",
    "    b = biasVar([128])\n",
    "    conv = conv2d(pool1, W)\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    conv2 = tf.nn.relu(out)\n",
    "    pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME', name='pool1')\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer2/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer2/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 3\n",
    "input: pool2 from convolution 2  \n",
    "filter: [3,3,128,256]  \n",
    "bias: 256  \n",
    "normalization: batch normalization  \n",
    "activation: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv3 - w/batch-norm (as source code, not paper)\n",
    "with tf.variable_scope('layer3'):\n",
    "    W = weightVar([3, 3, 128, 256])\n",
    "    b = biasVar([256])\n",
    "    conv = conv2d(pool2, W)\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                           training=is_training, name='batch-norm')\n",
    "    conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer3/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer3/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 4\n",
    "input: conv3 from convolution 3  \n",
    "filter: [3,3,256,256]  \n",
    "bias: 256  \n",
    "activation: relu  \n",
    "pooling: [1,2,2,1], [1,2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv4 - maxPool 2x1\n",
    "with tf.variable_scope('layer4'):\n",
    "    W = weightVar([3, 3, 256, 256])\n",
    "    b = biasVar([256])\n",
    "    conv = conv2d(conv3, W)\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    conv4 = tf.nn.relu(out)\n",
    "    pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                           padding='SAME', name='pool4')\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer4/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer4/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 5\n",
    "input: pool4 from convolution 4  \n",
    "filter: [3,3,256,512]  \n",
    "bias: 512  \n",
    "normalization: batch normalization  \n",
    "activation: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv5 - w/batch-norm\n",
    "with tf.variable_scope('layer5'):\n",
    "    W = weightVar([3, 3, 256, 512])\n",
    "    b = biasVar([512])\n",
    "    conv = conv2d(pool4, W)\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                           training=is_training, name='batch-norm')\n",
    "    conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer5/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer5/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 6\n",
    "input: conv5 from convolution 5  \n",
    "filter: [3,3,512,512]  \n",
    "bias: 512  \n",
    "activation: relu  \n",
    "pooling: [1,2,2,1], [1,2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv6 - maxPool 2x1 (as source code, not paper)\n",
    "with tf.variable_scope('layer6'):\n",
    "    W = weightVar([3, 3, 512, 512])\n",
    "    b = biasVar([512])\n",
    "    conv = conv2d(conv5, W)\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    conv6 = tf.nn.relu(out)\n",
    "    pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                           padding='SAME', name='pool6')\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer6/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer6/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolution layer 7\n",
    "input: pool6 from convolution 6  \n",
    "filter: [3,3,512,512]  \n",
    "bias: 512  \n",
    "normalization: batch normalization  \n",
    "activation: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - conv 7 - w/batch-norm (as source code, not paper)\n",
    "with tf.variable_scope('layer7'):\n",
    "    W = weightVar([2, 2, 512, 512])\n",
    "    b = biasVar([512])\n",
    "    conv = conv2d(pool6, W, padding='VALID')\n",
    "    out = tf.nn.bias_add(conv, b)\n",
    "    b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                           training=is_training, name='batch-norm')\n",
    "    conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "    if summaries:\n",
    "        weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer7/weights:0'][0]\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer7/bias:0'][0]\n",
    "        tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final network output\n",
    "input: conv7 from convolution 7  \n",
    "reshape from [batch, height, width, features] to [batch, width, height x features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_net = conv7\n",
    "\n",
    "with tf.variable_scope('Reshaping_cnn'):\n",
    "    shape = cnn_net.get_shape().as_list()  # [batch, height, width, features]\n",
    "    transposed = tf.transpose(cnn_net, perm=[0, 2, 1, 3],\n",
    "                              name='transposed')  # [batch, width, height, features]\n",
    "    conv_reshaped = tf.reshape(transposed, [shape[0], -1, shape[1] * shape[3]],\n",
    "                               name='reshaped')  # [batch, width, height x features]\n",
    "\n",
    "return conv_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to crnn function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logprob, raw_pred = deep_bidirectional_lstm(conv, params=parameters, summaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to deep_bidirectional_lstm***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bidirectional rnn from 2 basic LSTM cells with 2 layers size [256, 256]  \n",
    "input: output from deep convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def deep_bidirectional_lstm(inputs: tf.Tensor, params: Params, summaries: bool=True) -> tf.Tensor:\n",
    "# Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "# Current data input shape: (batch_size, n_steps, n_input) \"(batch, time, height)\"\n",
    "\n",
    "list_n_hidden = [256, 256]\n",
    "\n",
    "with tf.name_scope('deep_bidirectional_lstm'):\n",
    "    # Forward direction cells\n",
    "    fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "    # Backward direction cells\n",
    "    bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "    lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                    bw_cell_list,\n",
    "                                                                    inputs,\n",
    "                                                                    dtype=tf.float32\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout layer based on dropout probability in parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Dropout layer\n",
    "    lstm_net = tf.nn.dropout(lstm_net, keep_prob=params.keep_prob_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the rnn from [batch, width, 2\\*n_hidden] to [batch x width, 2\\*n_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    with tf.variable_scope('Reshaping_rnn'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fully connected layer with linear function, f:|reshaped rnn| -> # classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    with tf.variable_scope('fully_connected'):\n",
    "        W = weightVar([list_n_hidden[-1]*2, params.n_classes])\n",
    "        b = biasVar([params.n_classes])\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "\n",
    "        if summaries:\n",
    "            weights = [var for var in tf.global_variables()\n",
    "                       if var.name == 'deep_bidirectional_lstm/fully_connected/weights:0'][0]\n",
    "            tf.summary.histogram('weights', weights)\n",
    "            bias = [var for var in tf.global_variables()\n",
    "                    if var.name == 'deep_bidirectional_lstm/fully_connected/bias:0'][0]\n",
    "            tf.summary.histogram('bias', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape fully connected output and run through softmax to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    lstm_out = tf.reshape(fc_out, [shape[0], -1, params.n_classes], name='reshape_out')  # [batch, width, n_classes]\n",
    "\n",
    "    raw_pred = tf.argmax(tf.nn.softmax(lstm_out), axis=2, name='raw_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other output is changing the lstm output to dim [width(time), batch, n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Swap batch and time axis\n",
    "    lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "\n",
    "    return lstm_out, raw_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to crnn function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute seq_len from image width\n",
    "n_pools = CONST.DIMENSION_REDUCTION_W_POOLING  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide(features['images_widths'], n_pools, name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': logprob,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "try:\n",
    "    predictions_dict['filenames'] = features['filenames']\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get keys (letters) and values (integer stand ins for letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Alphabet and codes\n",
    "    keys = [c for c in parameters.alphabet] # the letters themselves\n",
    "    values = parameters.alphabet_codes # integer representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create non-string labels from the keys and values above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Convert string label to code label\n",
    "    with tf.name_scope('str2code_conversion'):\n",
    "        table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "        splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "        codes = table_str2int.lookup(splited.values)\n",
    "        sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "    seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                     minlength=tf.shape(predictions_dict['prob'])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ctc loss on probabilities from lstm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Loss\n",
    "    # ----\n",
    "    # >>> Cannot have longer labels than predictions -> error\n",
    "    with tf.control_dependencies([tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "        loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                                  inputs=predictions_dict['prob'],\n",
    "                                  sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                  preprocess_collapse_repeated=False,\n",
    "                                  ctc_merge_repeated=True,\n",
    "                                  ignore_longer_outputs_than_inputs=True,  # returns zero gradient in case it happens -> ema loss = NaN\n",
    "                                  time_major=True)\n",
    "        loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "        loss_ctc = tf.Print(loss_ctc, [loss_ctc], message='* Loss : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the learning rate as well as a moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    # # Create an ExponentialMovingAverage object\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=global_step, zero_debias=True)\n",
    "    # Create the shadow variables, and add op to maintain moving averages\n",
    "    maintain_averages_op = ema.apply([loss_ctc])\n",
    "    loss_ema = ema.average(loss_ctc)\n",
    "\n",
    "    # Train op\n",
    "    # --------\n",
    "    learning_rate = tf.train.exponential_decay(parameters.learning_rate, global_step,\n",
    "                                               parameters.learning_decay_steps, parameters.learning_decay_rate,\n",
    "                                               staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if parameters.optimizer == 'ada':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "    elif parameters.optimizer == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "    elif parameters.optimizer == 'rms':\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    opt_op = optimizer.minimize(loss_ctc, global_step=global_step)\n",
    "    with tf.control_dependencies(update_ops + [opt_op]):\n",
    "        train_op = tf.group(maintain_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for words (not totally necessary for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Summaries\n",
    "    # ---------\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    tf.summary.scalar('losses/ctc_loss', loss_ctc)\n",
    "else:\n",
    "    loss_ctc, train_op = None, None\n",
    "\n",
    "if mode in [tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.TRAIN]:\n",
    "    with tf.name_scope('code2str_conversion'):\n",
    "        keys = tf.cast(parameters.alphabet_decoding_codes, tf.int64)\n",
    "        values = [c for c in parameters.alphabet_decoding]\n",
    "        table_int2str = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), '?')\n",
    "\n",
    "        sparse_code_pred, log_probability = tf.nn.ctc_beam_search_decoder(predictions_dict['prob'],\n",
    "                                                                          sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                                                          merge_repeated=False,\n",
    "                                                                          beam_width=100,\n",
    "                                                                          top_paths=2)\n",
    "        # Score\n",
    "        predictions_dict['score'] = tf.subtract(log_probability[:, 0], log_probability[:, 1])\n",
    "        # around 10.0 -> seems pretty sure, less than 5.0 bit unsure, some errors/challenging images\n",
    "        sparse_code_pred = sparse_code_pred[0]\n",
    "\n",
    "        sequence_lengths_pred = tf.bincount(tf.cast(sparse_code_pred.indices[:, 0], tf.int32),\n",
    "                                            minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "        pred_chars = table_int2str.lookup(sparse_code_pred)\n",
    "        predictions_dict['words'] = get_words_from_chars(pred_chars.values, sequence_lengths=sequence_lengths_pred)\n",
    "\n",
    "        tf.summary.text('predicted_words', predictions_dict['words'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something for eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation ops\n",
    "# --------------\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    with tf.name_scope('evaluation'):\n",
    "        CER = tf.metrics.mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "\n",
    "        # Convert label codes to decoding alphabet to compare predicted and groundtrouth words\n",
    "        target_chars = table_int2str.lookup(tf.cast(sparse_code_target, tf.int64))\n",
    "        target_words = get_words_from_chars(target_chars.values, seq_lengths_labels)\n",
    "        accuracy = tf.metrics.accuracy(target_words, predictions_dict['words'], name='accuracy')\n",
    "\n",
    "        eval_metric_ops = {\n",
    "                           'eval/accuracy': accuracy,\n",
    "                           'eval/CER': CER,\n",
    "                           }\n",
    "        CER = tf.Print(CER, [CER], message='-- CER : ')\n",
    "        accuracy = tf.Print(accuracy, [accuracy], message='-- Accuracy : ')\n",
    "\n",
    "else:\n",
    "    eval_metric_ops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the model for the estimator above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_outputs = {'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "\n",
    "return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions_dict,\n",
    "    loss=loss_ctc,\n",
    "    train_op=train_op,\n",
    "    eval_metric_ops=eval_metric_ops,\n",
    "    export_outputs=export_outputs,\n",
    "    scaffold=tf.train.Scaffold()\n",
    "    # scaffold=tf.train.Scaffold(init_fn=None)  # Specify init_fn to restore from previous model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
