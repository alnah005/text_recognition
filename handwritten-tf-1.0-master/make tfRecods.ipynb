{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "from __future__ import absolute_import, division, print_function\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import skimage.util\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import math\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary = {}\n",
    "# vocabulary_path = \"some.mat\"\n",
    "def getIndex(c,voc):\n",
    "    for name, age in voc.iteritems():\n",
    "        if age == c:\n",
    "            return name\n",
    "    print(\"-\"*30,\"error-\",c)\n",
    "    return None\n",
    "\n",
    "# if os.path.exists(vocabulary_path):\n",
    "#     imgs=scipy.io.loadmat(vocabulary_path)['vocabulary']\n",
    "# else:\n",
    "#     nrC=1\n",
    "#     vocabulary['%%'] = 0 \n",
    "#     '''\n",
    "#     c = '0'\n",
    "    \n",
    "#     while ord(c) != ord('9')+1:\n",
    "#         vocabulary[c] = nrC\n",
    "#         nrC = nrC + 1\n",
    "#         c = chr(ord(c)+1)\n",
    "#     c = 'A'\n",
    "#     while ord(c) != ord('Z')+1:\n",
    "#         vocabulary[c] = nrC\n",
    "#         nrC = nrC + 1\n",
    "#         c = chr(ord(c)+1)\n",
    "#     '''\n",
    "#     c = 'a'\n",
    "#     while ord(c) != ord('z')+1:\n",
    "#         vocabulary[c] = nrC\n",
    "#         nrC = nrC + 1\n",
    "#         c = chr(ord(c)+1)\n",
    "#     '''\n",
    "#     cr = [',','.','\"','\\'',' ','-','#','(',')',';','?',':','*','&','!','/','']\n",
    "#     for c in cr:\n",
    "#         vocabulary[c] = nrC\n",
    "#         nrC = nrC + 1\n",
    "#     '''\n",
    "#     vocabulary[' '] = nrC\n",
    "#     nrC += 1\n",
    "#     vocabulary[''] = nrC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = {' ': 0, '!': 1, '\"': 2, '#': 3, '&': 4, \"'\": 5, '(': 6, ')': 7,\n",
    "              '*': 8, '+': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14,\n",
    "              '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21,\n",
    "              '8': 22, '9': 23, ':': 24, ';': 25, '<': 26, '=': 27, '>': 28,\n",
    "              '?': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35,\n",
    "              'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42,\n",
    "              'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49,\n",
    "              'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, '[': 55, ']': 56,\n",
    "              '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63,\n",
    "              'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70,\n",
    "              'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77,\n",
    "              'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '|': 84,\n",
    "              '~': 85, '': 86}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessTarget(line):\n",
    "    \n",
    "    QUOT_TOKEN_Init = \"&quot;\"\n",
    "    QUOT_TOKEN = '\"'\n",
    "           \n",
    "    FIRST_INDEX = ord('a') - 4  # 0 is reserved to space\n",
    "\n",
    "    targets = line.strip().replace(QUOT_TOKEN_Init, QUOT_TOKEN)\n",
    "        \n",
    "    targets = np.asarray([vocabulary[x]  for x in targets])\n",
    "    \n",
    "    return targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxL = 100#inaltimea\n",
    "maxi=0\n",
    "def processPage(nameXml,nameImg,words=False,maxW=400):\n",
    "    img = cv2.imread(nameImg, cv2.IMREAD_GRAYSCALE )\n",
    "    text = open(nameXml, \"r\").readline().replace(\"\\n\", \"\")\n",
    "    if any([ord(i) > 127 for i in text]):\n",
    "        return [None, None, None]\n",
    "    \n",
    "    global maxi\n",
    "    letters = list()\n",
    "    coords = list()\n",
    "    imgs = []\n",
    "    texts = []\n",
    "    lengths = []\n",
    "\n",
    "#     img = form[int(line.attrib['asy']):int(line.attrib['dsy'])]\n",
    "    ret,thresh1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "#     maxi=max(maxi,img.shape[0])\n",
    "    texts.append(preprocessTarget(text))\n",
    "    imgs.append(thresh1)\n",
    "    lengths.append(thresh1.shape[1])\n",
    "        \n",
    "    return [texts,imgs,lengths]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nameXml = '../data_in/iamHandwriting/xml/b05-074.xml'\n",
    "# nameImg = '../data_in/iamHandwriting/forms/b05-074.png'#'forms/a01-030u.png'#'forms/a01-113.png'\n",
    "nameXml = \"../data/Transcriptions/002_080_001_03_07.txt\"\n",
    "nameImg = \"../data/Images_mod/002_080_001_03_07.png\"\n",
    "\n",
    "t=processPage(nameXml,nameImg,False)\n",
    "#t[0]\n",
    "#print (maxi)\n",
    "print (len(t[0]))\n",
    "#for e,i in enumerate(t[0]):\n",
    "#    print(decodePrediction([[],t[0][e]]))\n",
    "#plt.imshow(t[1][0])\n",
    "l = 0;h = 0\n",
    "plt.figure(figsize=(15,25))\n",
    "for e in range(len(t[0][-9*2:])):\n",
    "    l = max(l,len(t[1][e][:,0]))\n",
    "    h = max(h,len(t[1][e][0]))\n",
    "    plt.subplot(9,2,e+1)\n",
    "    plt.imshow(t[1][e])\n",
    "l,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features(img, thesh=100):\n",
    "    l = [img]\n",
    "    for i in range(3):\n",
    "        l.append(cv2.Sobel(img,cv2.CV_64F,1,0,ksize=(i*2+1)))\n",
    "        l.append(cv2.Sobel(img,cv2.CV_64F,0,1,ksize=(i*2+1)))\n",
    "    for i in range(1):\n",
    "        l.append(cv2.Canny(img,100+10*i,200))\n",
    "    for i in range(3):#block size, ksize, k harris\n",
    "        for j in range(3):\n",
    "                l.append(cv2.cornerHarris(img,2+j,(i*2+1),5))\n",
    "    theta, bwimg = cv2.threshold(img, 100, 255, cv2.THRESH_OTSU)\n",
    "    l.append(bwimg)\n",
    "    #l.append(correctSlant(img,tt=False))\n",
    "    l = [(i-np.amin(i))/(np.amax(i)-np.amin(i)) for i in l]\n",
    "    return l\n",
    "    #p = [np.expand_dims(preprocessing.scale(np.float32(i)),2) for i in l]\n",
    "    #return np.concatenate(p,2)\n",
    "imgs = get_features(t[1][0])\n",
    "#print(imgs.shape)\n",
    "plt.figure(figsize=(15,25))\n",
    "for i  in range(len(imgs)):\n",
    "    plt.subplot(20,2,i+1),plt.imshow(imgs[i],cmap = 'gray')\n",
    "    plt.title('{}'.format(i)), plt.xticks([]), plt.yticks([]), colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSlices(img,w,stride=3,W=130,H=36):\n",
    "    img = 1.0 - img\n",
    "    assert w%2==0\n",
    "    w1 = img.shape[1]\n",
    "    ll = 100\n",
    "    l = []\n",
    "    #imgr = resize(img,height=H)\n",
    "    #if imgr.shape[1]<=W:\n",
    "    #    img = imgr\n",
    "    if W!=None and H!=None:\n",
    "        if img.shape[0]!=H and int((H-img.shape[0])/2)>0:\n",
    "            img = np.insert(img, [0]*int((H-img.shape[0])/2), 0.5, axis=0)\n",
    "        if img.shape[0]!=H:\n",
    "            img = np.insert(img, [img.shape[0]]*(H-img.shape[0]), 0.5, axis=0)\n",
    "        if img.shape[1]!=W:\n",
    "            img = np.insert(img, [img.shape[1]]*(W-img.shape[1]), 0.5, axis=1)    \n",
    "    if img.shape[1]%w!=0 and False:\n",
    "        img = np.insert(img, [img.shape[1]]*(w-(img.shape[1]%w)), 0.5, axis=1)\n",
    "    i = 0\n",
    "    #return img, w1\n",
    "    #add begining and ending offset\n",
    "    #img = np.insert(img, [img.shape[1]]*(int(w*0.6)), 0, axis=1)  \n",
    "    #img = np.insert(img, [0]*(int(w*0.6)), 0, axis=1)  \n",
    "    \n",
    "    #make slices\n",
    "    \n",
    "    cosineWindow = [signal.cosine(w) for i in range(H)]\n",
    "    while i<img.shape[1]-w:\n",
    "        if i>w1 and ll == 100:\n",
    "            ll = len(l)\n",
    "        #apply cosine transform\n",
    "        frame = img[:,i:i+w]\n",
    "        #frame = frame*np.array(cosineWindow)      \n",
    "        l.append(frame)\n",
    "        i=i+3\n",
    "    if len(np.asarray(l)) < ll+1:\n",
    "        ll = len(np.asarray(l))-1\n",
    "    return np.asarray(l), ll+1#len is not correct!!\n",
    "\n",
    "def getSlicesImgs(imgs,w):\n",
    "    nImgs = []\n",
    "    nln = []\n",
    "    for img in imgs:\n",
    "        #correct slant\n",
    "        #aa = findAngle(img,thesh=100)\n",
    "        #print(np.amin(img))\n",
    "        #print(aa)\n",
    "        #img = shear(img,aa,binar=False)\n",
    "        #print(np.amin(img))\n",
    "        #get filters\n",
    "        imgT = get_features(img)\n",
    "        #print(imgT[0].shape,len(imgT))\n",
    "        \n",
    "        ims = []\n",
    "        for ii in imgT:\n",
    "            im, ln = getSlices(ii,w)\n",
    "            ims.append(im*-1.)\n",
    "        #print(ims[0].shape,len(ims))\n",
    "        \n",
    "        nln.append(ln)\n",
    "        ims = [np.expand_dims(np.float32(i),3) for i in ims]\n",
    "        ims = np.concatenate(ims,3)\n",
    "        #print(ims.shape)\n",
    "        nImgs.append(ims)\n",
    "        #return 0\n",
    "    return nImgs, nln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,k = getSlicesImgs(t[1],12)\n",
    "print(i[0].shape,t[1][0].shape)\n",
    "print(k)\n",
    "plt.imshow(i[0][3,:,:,0]);plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def addPaddingTF(imgs):\n",
    "    imgs = [imgs]\n",
    "    at = []\n",
    "    H=40#120#347\n",
    "    W = 200#300\n",
    "    for k,img1 in enumerate(imgs):\n",
    "        img = imgs[k]\n",
    "        if img.shape[0]!=H and int((H-img.shape[0])/2)>0:\n",
    "            img = np.insert(img, [0]*int((H-img.shape[0])/2), 0, axis=0)\n",
    "        if img.shape[0]!=H:\n",
    "            img = np.insert(img, [img.shape[0]]*(H-img.shape[0]), 0, axis=0)\n",
    "        if img.shape[1]!=W:\n",
    "            img = np.insert(img, [img.shape[1]]*(W-img.shape[1]), 0, axis=1)\n",
    "        at.append(img)\n",
    "    return at[0]\n",
    "\n",
    "def makeBatchesTFRecord(pathSave,pathXML,pathImgs,batchSize,words=False):\n",
    "    if not os.path.exists(pathSave):\n",
    "        os.makedirs(pathSave)\n",
    "    totalT, totalI, totalL = [],[],[]\n",
    "    nr=0\n",
    "    maxle=0\n",
    "    txtLD = {}\n",
    "    dd,ll=0,0\n",
    "    \n",
    "    filenames = [f.replace(\".txt\", \"\") for f in os.listdir(pathXML)]\n",
    "    \n",
    "    count = 0\n",
    "    for name in sorted(filenames):\n",
    "#         if count % 500 == 0: print(count, end=\"\\t\")\n",
    "#         count += 1\n",
    "        \n",
    "        xml = pathXML + name + \".txt\"\n",
    "#         name = xml.split('.txt')[0].split('/')[-1]\n",
    "        img = pathImgs + name + '.png'\n",
    "#         print(img)\n",
    "        if img == \"forms/g06-011p.png\":\n",
    "            break\n",
    "\n",
    "        texts,imgs,lengths = processPage(xml,img,words)\n",
    "        if texts is None or imgs is None or lengths is None:\n",
    "            continue\n",
    "\n",
    "\n",
    "        imgs,lengths = getSlicesImgs(imgs,24)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        totalT.extend(texts)\n",
    "        totalI.extend(imgs)\n",
    "        totalL.extend(lengths)\n",
    "\n",
    "        if(len(totalT)>=batchSize):\n",
    "            #save batch\n",
    "            print(nr,'-l',maxle)\n",
    "            if True:\n",
    "                writer = tf.python_io.TFRecordWriter(pathSave+\"handwritten-test-{}.tfrecords\".format(nr), tf.python_io.TFRecordOptions(2))\n",
    "                for ii in range(batchSize):\n",
    "                    print(ii, 1, end = \"\\t\")\n",
    "#                     if totalI[ii].shape[0] < 50 or totalI[ii].shape[0] > 200 or \\\n",
    "#                     totalI[ii].shape[1]<10 or totalI[ii].shape[1]>40:\n",
    "#                         print('error!!')\n",
    "#                         continue\n",
    "                    imgI = addPaddingTF(totalI[ii])  \n",
    "                    print(ii, 2, end = \"\\t\")  \n",
    "                    # construct the Example proto boject\n",
    "                    example = tf.train.Example(\n",
    "                        # Example contains a Features proto object\n",
    "                        features=tf.train.Features(\n",
    "                          # Features contains a map of string to Feature proto objects\n",
    "                          feature={\n",
    "                            # A Feature contains one of either a int64_list,\n",
    "                            # float_list, or bytes_list\n",
    "                            'seq_len': tf.train.Feature(\n",
    "                                int64_list=tf.train.Int64List(value=[totalL[ii]])),\n",
    "                            'target': tf.train.Feature(\n",
    "                                int64_list=tf.train.Int64List(value=totalT[ii].astype(\"int64\"))),\n",
    "                            'imageInput': tf.train.Feature(\n",
    "                                float_list=tf.train.FloatList(value=(imgI).reshape(-1).astype(\"float\"))),\n",
    "                    }))\n",
    "                    print(ii, 3, end = \"\\t\")\n",
    "                    # use the proto object to serialize the example to a string\n",
    "                    serialized = example.SerializeToString()\n",
    "                    print(ii, 4, end = \"\\t\")\n",
    "                    # write the serialized object to disk\n",
    "                    writer.write(serialized)\n",
    "                    print(ii, 5)\n",
    "                writer.close()\n",
    "            nr += 1  \n",
    "            print(nr,'the file saved!')\n",
    "            clear_output()\n",
    "            totalT = totalT[batchSize:]\n",
    "            totalI = totalI[batchSize:]\n",
    "            totalL = totalL[batchSize:]\n",
    "    target = open(pathSave+'readme.txt', 'w')\n",
    "    target.write(\"max len text:\"+str(maxle)+\"\\n\")\n",
    "    target.write(\"image shape:50x300 \\n\")\n",
    "    target.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "makeBatchesTFRecord('../data/batch/','../data/Transcriptions/',\n",
    "                    '../data/Images_mod/',20,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
