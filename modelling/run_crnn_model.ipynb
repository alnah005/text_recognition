{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "import pickle\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from .decoding import get_words_from_chars\n",
    "# from .config import Params, CONST\n",
    "# from src.model import crnn_fn\n",
    "# from src.data_handler import data_loader\n",
    "# from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "# from src.config import Params, Alphabet, import_params_from_json\n",
    "\n",
    "csv_files_train = \"../data/train.csv\"\n",
    "csv_files_eval = \"../data/valid.csv\"\n",
    "output_model_dir = \"./estimator\"\n",
    "output_graph_dir = \"./graph\"\n",
    "if not os.path.isdir(output_model_dir):\n",
    "    os.mkdir(output_model_dir)\n",
    "if not os.path.isdir(output_graph_dir):\n",
    "    os.mkdir(output_graph_dir)\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "gpu = \"\" # help=\"GPU 0,1 or '' \", default=''\n",
    "\n",
    "train_batch_size=64\n",
    "eval_batch_size=64\n",
    "# train_batch_size=10\n",
    "# eval_batch_size=10\n",
    "learning_rate=1e-3  # 1e-3 recommended\n",
    "learning_decay_rate=0.95\n",
    "learning_decay_steps=5000\n",
    "evaluate_every_epoch=5\n",
    "save_interval=5e3\n",
    "input_shape=(129, 369) #iam shape\n",
    "# input_shape=(117, 1669) # bentham shape1\n",
    "# input_shape=(181, 1833) # bentham shape2\n",
    "optimizer='adam'\n",
    "digits_only=False\n",
    "# alphabet=\" !\\\"#&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|\"\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "alphabet_decoding='same'\n",
    "alphabet_codes = list(range(len(alphabet)))\n",
    "n_classes = len(alphabet)\n",
    "csv_delimiter='\\t'\n",
    "keep_prob_dropout = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed for quickly making convolutional layers\n",
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_tensor = features['images']\n",
    "input_tensor = tf.placeholder(tf.float32, [None, input_shape[0], input_shape[1], 1])\n",
    "labels = tf.placeholder(tf.string, [None])\n",
    "is_training = True\n",
    "\n",
    "if input_tensor.shape[-1] == 1:\n",
    "    input_channels = 1\n",
    "elif input_tensor.shape[-1] == 3:\n",
    "    input_channels = 3\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('deep_cnn'):\n",
    "    # conv1 - maxPool2x2\n",
    "    with tf.variable_scope('layer1'):\n",
    "        W = weightVar([3, 3, input_channels, 64])\n",
    "        b = biasVar([64])\n",
    "        conv = conv2d(input_tensor, W, name='conv')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv1 = tf.nn.relu(out)\n",
    "        pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "\n",
    "    # conv2 - maxPool 2x2\n",
    "    with tf.variable_scope('layer2'):\n",
    "        W = weightVar([3, 3, 64, 128])\n",
    "        b = biasVar([128])\n",
    "        conv = conv2d(pool1, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv2 = tf.nn.relu(out)\n",
    "        pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "    # conv3 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer3'):\n",
    "        W = weightVar([3, 3, 128, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(pool2, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "    # conv4 - maxPool 2x1\n",
    "    with tf.variable_scope('layer4'):\n",
    "        W = weightVar([3, 3, 256, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(conv3, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv4 = tf.nn.relu(out)\n",
    "        pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool4')\n",
    "\n",
    "    # conv5 - w/batch-norm\n",
    "    with tf.variable_scope('layer5'):\n",
    "        W = weightVar([3, 3, 256, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool4, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # conv6 - maxPool 2x1 (as source code, not paper)\n",
    "    with tf.variable_scope('layer6'):\n",
    "        W = weightVar([3, 3, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(conv5, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv6 = tf.nn.relu(out)\n",
    "        pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool6')\n",
    "\n",
    "    # conv 7 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer7'):\n",
    "        W = weightVar([2, 2, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool6, W, padding='VALID')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # reshape output\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = conv7.get_shape().as_list()  # [batch, height, width, features]\n",
    "        shape_tens = tf.shape(conv7)\n",
    "        transposed = tf.transpose(conv7, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_out = tf.reshape(transposed, [shape_tens[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logprob, raw_pred = deep_bidirectional_lstm(conv, params=parameters, summaries=False)\n",
    "\n",
    "# def deep_bidirectional_lstm(inputs: tf.Tensor, params: Params, summaries: bool=True) -> tf.Tensor:\n",
    "# Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "# Current data input shape: (batch_size, n_steps, n_input) \"(batch, time, height)\"\n",
    "\n",
    "list_n_hidden = [256, 256]\n",
    "\n",
    "with tf.name_scope('deep_bidirectional_lstm'):\n",
    "    # Forward direction cells\n",
    "    fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "    # Backward direction cells\n",
    "    bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "    lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                    bw_cell_list,\n",
    "                                                                    conv_out, # THE INPUT\n",
    "                                                                    dtype=tf.float32\n",
    "                                                                    )\n",
    "\n",
    "    # Dropout layer\n",
    "    lstm_net = tf.nn.dropout(lstm_net, keep_prob=keep_prob_dropout)\n",
    "\n",
    "    with tf.variable_scope('Reshaping_rnn'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]\n",
    "\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        W = weightVar([list_n_hidden[-1]*2, n_classes])\n",
    "        b = biasVar([n_classes])\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "\n",
    "    shape_tens = tf.shape(lstm_net)\n",
    "    lstm_out = tf.reshape(fc_out, [shape_tens[0], -1, n_classes], name='reshape_out')  # [batch, width, n_classes]\n",
    "\n",
    "    raw_pred = tf.argmax(tf.nn.softmax(lstm_out), axis=2, name='raw_prediction')\n",
    "\n",
    "    # Swap batch and time axis\n",
    "    lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up for loss and training\n",
    "\n",
    "# Compute seq_len from image width\n",
    "n_pools = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide([input_shape[1]]*train_batch_size, n_pools,\n",
    "                           name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': lstm_out,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "# Get keys (letters) and values (integer stand ins for letters)\n",
    "# Alphabet and codes\n",
    "keys = [c for c in alphabet] # the letters themselves\n",
    "values = alphabet_codes # integer representations\n",
    "\n",
    "\n",
    "# Create non-string labels from the keys and values above\n",
    "# Convert string label to code label\n",
    "with tf.name_scope('str2code_conversion'):\n",
    "    table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "    splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "    codes = table_str2int.lookup(splited.values)\n",
    "    sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                 minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "\n",
    "# Use ctc loss on probabilities from lstm output\n",
    "# Loss\n",
    "# ----\n",
    "# >>> Cannot have longer labels than predictions -> error\n",
    "with tf.control_dependencies([tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "    loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                              inputs=predictions_dict['prob'],\n",
    "                              sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                              preprocess_collapse_repeated=False,\n",
    "                              ctc_merge_repeated=True,\n",
    "                              ignore_longer_outputs_than_inputs=True,  # returns zero gradient in case it happens -> ema loss = NaN\n",
    "                              time_major=True)\n",
    "    loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "    loss_ctc = tf.Print(loss_ctc, [loss_ctc], message='* Loss : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_chars(characters_list: List[str], sequence_lengths: List[int], name='chars_conversion'):\n",
    "    with tf.name_scope(name=name):\n",
    "        def join_charcaters_fn(coords):\n",
    "            return tf.reduce_join(characters_list[coords[0]:coords[1]])\n",
    "\n",
    "        def coords_several_sequences():\n",
    "            end_coords = tf.cumsum(sequence_lengths)\n",
    "            start_coords = tf.concat([[0], end_coords[:-1]], axis=0)\n",
    "            coords = tf.stack([start_coords, end_coords], axis=1)\n",
    "            coords = tf.cast(coords, dtype=tf.int32)\n",
    "            return tf.map_fn(join_charcaters_fn, coords, dtype=tf.string)\n",
    "\n",
    "        def coords_single_sequence():\n",
    "            return tf.reduce_join(characters_list, keep_dims=True)\n",
    "\n",
    "        words = tf.cond(tf.shape(sequence_lengths)[0] > 1,\n",
    "                        true_fn=lambda: coords_several_sequences(),\n",
    "                        false_fn=lambda: coords_single_sequence())\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('code2str_conversion'):\n",
    "    keys = tf.cast(alphabet_codes, tf.int64)\n",
    "    values = [c for c in alphabet]\n",
    "\n",
    "    table_int2str = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), '?')\n",
    "\n",
    "    sparse_code_pred, log_probability = tf.nn.ctc_beam_search_decoder(predictions_dict['prob'],\n",
    "                                                                      sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                                                      merge_repeated=False,\n",
    "                                                                      beam_width=100,\n",
    "                                                                      top_paths=2)\n",
    "    # Score\n",
    "    predictions_dict['score'] = tf.subtract(log_probability[:, 0], log_probability[:, 1])\n",
    "    # around 10.0 -> seems pretty sure, less than 5.0 bit unsure, some errors/challenging images\n",
    "    sparse_code_pred = sparse_code_pred[0]\n",
    "\n",
    "    sequence_lengths_pred = tf.bincount(tf.cast(sparse_code_pred.indices[:, 0], tf.int32),\n",
    "                                        minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "    pred_chars = table_int2str.lookup(sparse_code_pred)\n",
    "    predictions_dict['words'] = get_words_from_chars(pred_chars.values, sequence_lengths=sequence_lengths_pred)\n",
    "\n",
    "    tf.summary.text('predicted_words', predictions_dict['words'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    CER = tf.metrics.mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "    CER = tf.reduce_mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "\n",
    "    # Convert label codes to decoding alphabet to compare predicted and groundtrouth words\n",
    "    target_chars = table_int2str.lookup(tf.cast(sparse_code_target, tf.int64))\n",
    "    target_words = get_words_from_chars(target_chars.values, seq_lengths_labels)\n",
    "    accuracy = tf.metrics.accuracy(target_words, predictions_dict['words'], name='accuracy')\n",
    "\n",
    "    eval_metric_ops = {\n",
    "                       'eval/accuracy': accuracy,\n",
    "                       'eval/CER': CER,\n",
    "                       }\n",
    "    CER = tf.Print(CER, [CER], message='-- CER : ')\n",
    "    accuracy = tf.Print(accuracy, [accuracy], message='-- Accuracy : ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the learning rate as well as a moving average\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# # Create an ExponentialMovingAverage object\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=global_step, zero_debias=True)\n",
    "# Create the shadow variables, and add op to maintain moving averages\n",
    "maintain_averages_op = ema.apply([loss_ctc])\n",
    "loss_ema = ema.average(loss_ctc)\n",
    "\n",
    "# Train op\n",
    "# --------\n",
    "learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                           learning_decay_steps, learning_decay_rate,\n",
    "                                           staircase=True)\n",
    "\n",
    "# Set up optimizer\n",
    "if optimizer == 'ada':\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "elif optimizer == 'rms':\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "opt_op = optimizer.minimize(loss_ctc, global_step=global_step)\n",
    "with tf.control_dependencies(update_ops + [opt_op]):\n",
    "    train_op = tf.group(maintain_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the details to make all images the same size\n",
    "\n",
    "with open(csv_files_train, \"r\") as f:\n",
    "    f_lines = f.read().splitlines()[1:]\n",
    "    filenames = [l.split(\"\\t\")[0] for l in f_lines]\n",
    "    label_list = [l.split(\"\\t\")[1] for l in f_lines]\n",
    "\n",
    "datasize = len(label_list)\n",
    "filenames = tf.constant(filenames)\n",
    "label_list = tf.constant(label_list)\n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize_image_with_crop_or_pad(image_decoded,\n",
    "                                                           input_shape[0],\n",
    "                                                           input_shape[1])\n",
    "    return image_resized, label\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, label_list))\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.batch(train_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "losses = []\n",
    "cers = []\n",
    "accs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "#     saver.restore(sess, output_model_dir+\"/model.ckpt\")\n",
    "    \n",
    "    writer = tf.summary.FileWriter(output_graph_dir+'/crnn', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for j in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            try:\n",
    "                _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                              sparse_code_pred, pred_chars],\n",
    "                             feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "                losses.append(loss)\n",
    "                cers.append(cer)\n",
    "                accs.append(accs)\n",
    "                pickle.dump([cers, accs, losses, preddict, scp, pc], open(output_model_dir+\"/preds.pkl\", \"wb\"))\n",
    "                saver.save(sess, output_model_dir+\"/model.ckpt\")\n",
    "                \n",
    "                print('batch: {0}, loss: {3} \\n\\tCER: {1}, accuracy: {2}'.format(j, cer, acc, loss))\n",
    "            except:\n",
    "                print(\"Error at \", j)\n",
    "        print('Avg Epoch time: {0} seconds'.format((time.time() - start_time)/(1.0*(i+1))))\n",
    "            \n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try getting actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "labls = []\n",
    "losses = []\n",
    "cers = []\n",
    "accs = []\n",
    "preddicts = []\n",
    "scps = []\n",
    "pcs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, output_model_dir+\"/model.ckpt\")\n",
    "    \n",
    "    writer = tf.summary.FileWriter('./graph/crnn_pred', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    print(\"Starting epoch\")\n",
    "    sess.run(iterator.initializer)\n",
    "    for j in range(n_batches):\n",
    "        input_tensor_b, labels_b = sess.run(next_batch)\n",
    "        \n",
    "        try:\n",
    "            cer, acc, loss, preddict, scp, pc = sess.run([CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                          sparse_code_pred, pred_chars],\n",
    "                         feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            labls.append(labels_b)\n",
    "            losses.append(loss)\n",
    "            cers.append(cer)\n",
    "            accs.append(accs)\n",
    "            preddicts.append(preddict)\n",
    "            scps.append(scp)\n",
    "            pcs.append(pc)\n",
    "\n",
    "            pickle.dump([labls, accs, losses, cers, preddicts, scps, pcs], open(output_model_dir+\"/preds2.pkl\", \"wb\"))\n",
    "            print('batch: {0}, CER: {1}, accuracy: {2}, loss: {3}'.format(j, cer, acc, loss))\n",
    "        except:\n",
    "            labls.append([])\n",
    "            losses.append([])\n",
    "            cers.append([])\n",
    "            accs.append([])\n",
    "            preddicts.append([])\n",
    "            scps.append([])\n",
    "            pcs.append([])\n",
    "\n",
    "            pickle.dump([labls, accs, losses, cers, preddicts, scps, pcs], open(output_model_dir+\"/preds2.pkl\", \"wb\"))\n",
    "            print(\"Error at \", j)\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
